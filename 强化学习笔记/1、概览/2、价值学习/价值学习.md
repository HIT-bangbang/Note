# Value-based learning

## 1ã€ä¸»è¦æ€æƒ³

* ä½¿ç”¨ç¥ç»ç½‘ç»œ$Q(s,a;\mathbf{w})$è¿‘ä¼¼$Q^{\star}(s,a)$
* è§‚æµ‹åˆ°çŠ¶æ€$s$ï¼Œé€‰æ‹©ä½¿å¾—$Q^{\star}(s,a)$æœ€å¤§åŒ–çš„åŠ¨ä½œ$a^{\star}$ä½œä¸ºä¸‹ä¸€æ­¥çš„åŠ¨ä½œ
  $$a^{\star} = \argmax _{a}Q^{\star}(s,a) $$

## 2ã€Deep QNetwork (DQN)

* è¾“å…¥shapeï¼šå›¾ç‰‡çš„size
* è¾“å‡ºshape:åŠ¨ä½œç©ºé—´çš„ç»´åº¦dimension of action space
  
<img src="1.png" height=50% width=50%>

## 3ã€äº¤äº’è¿‡ç¨‹

<img src="2.png" height=50% width=50%>

## 4ã€æ—¶åºå·®åˆ†Temporal Difference (TD) Learning

## ä¾‹å­ï¼š
* I want to drive from NYC to Atlanta.
* Model $ğ‘„(\mathbf{w})$ estimates the time cost, e.g., 1000 minutes.
* Question: How do I update the model?


### method 1 éœ€è¦åˆ°è¾¾ç›®çš„åœ°

<img src="3.png" height=20% width=20%>

å‚æ•°$\mathbf{w}$æ›´æ–°æ­¥éª¤ï¼š

* Make a prediction using$ğ‘„(\mathbf{w})$ : $ğ‘ = ğ‘„(\mathbf{w})$, e.g., $ğ‘ = 1000$. 

* Finish the trip and get the target ğ‘¦, e.g., ğ‘¦ = 860.

* Get LOSS : $L=\frac{1}{2}(q-y)^{2}$

* Calculate Gradientï¼š$\frac{\partial L}{\partial \mathbf{w}}=\frac{\partial q}{\partial \mathbf{w}} \cdot \frac{\partial L}{\partial q}=(q-y) \cdot \frac{\partial Q(\mathbf{w})}{\partial \mathbf{w}}$
* Gradient descent:$\mathbf{w}_{t+1}=\mathbf{w}_t-\left.\alpha \cdot \frac{\partial L}{\partial \mathbf{w}}\right|_{\mathbf{w}=\mathbf{w}_t}$

### method 2 ä¸éœ€è¦åˆ°è¾¾ç›®çš„åœ°ï¼Œä¸­é€”åœä¸‹

<img src="4.png" height=20% width=20%>
<img src="5.png" height=20% width=20%>

* Can I update the model before finishing the trip?
* Can I get a better ğ° as soon as I arrived at DC?

å‚æ•°$\mathbf{w}$æ›´æ–°æ­¥éª¤ï¼š

* Modelâ€™s estimate:   
   * NYC to Atlanta: 1000 minutes (**estimate**).
   * $ğ‘„(\mathbf{w})=1000$  minutes.
* I arrived at DC; 
  * actual time cost: NYC to DC: 300 minutes (**actual**).
* Model now updates its estimate:
  * DC to Atlanta: 600 minutes (**estimate**).
  * Updated estimate: 300 + 600 = 900 minutes.(TD target)
* TD target $y = 900$ is a more reliable estimate than 1000

* Loss: 
$$
L=\frac{1}{2}(\underbrace{Q(\mathbf{w})-y}_{TD \ \ error})^2
$$

* Gradient:
$$
\frac{\partial L}{\partial \mathbf{w}}=\underbrace{(1000-900)}_{TD \ \ error} \cdot \frac{\partial Q(\mathbf{w})}{\partial \mathbf{w}}
$$

* Gradient descent: 
$$
\mathbf{w}_{t+1}=\mathbf{w}_t-\left.\alpha \cdot \frac{\partial L}{\partial \mathbf{w}}\right|_{\mathbf{w}=\mathbf{w}_t}
$$

<img src="6.png" height=50% width=50%>


# æ—¶åºå·®åˆ†æ³•åœ¨DQNä¸­çš„åº”ç”¨

$$
Q\left(s_t, a_t ; \mathbf{w}\right) \approx r_t+\gamma \cdot Q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)
$$

## æ¨å¯¼ï¼š

å¯¹äºæŠ˜æ‰£å›æŠ¥$U_t$ å’Œ $U_{t+1}$æœ‰è¿™æ ·çš„å…³ç³»ï¼š
$$
\begin{aligned}
U_t & =R_t+\gamma \cdot R_{t+1}+\gamma^2 \cdot R_{t+2}+\gamma^3 \cdot R_{t+3}+\gamma^4 \cdot R_{t+4}+\cdots \\
& =R_t+\gamma \cdot \underbrace{\left(R_{t+1}+\gamma \cdot R_{t+2}+\gamma^2 \cdot R_{t+3}+\gamma^3 \cdot R_{t+4}+\cdots\right)}_{=U_{t+1}}
\end{aligned}
$$
å³ï¼š
$$ U_t = R_t +\gamma \cdot U_{t+1} $$

åŒæ—¶ï¼š
* DQNçš„è¾“å‡º$ğ‘„(ğ‘ _t,ğ‘_t;\mathbf{w})$æ˜¯å¯¹$U_t$çš„ä¼°è®¡ï¼ˆæœŸæœ›ï¼‰
* DQNçš„è¾“å‡º$ğ‘„(ğ‘ _{t+1},ğ‘_{t+1};\mathbf{w})$æ˜¯å¯¹$U_{t+1}$çš„ä¼°è®¡ï¼ˆæœŸæœ›ï¼‰

æ‰€ä»¥ï¼Œç­‰å¼ä¸¤ä¾§å¯¹$S_{t+1}$å’Œ$A_{t+1}$æ±‚æœŸæœ›å¯å¾—ï¼š
$$
\underbrace{Q\left(s_t, a_t ; \mathbf{w}\right)}_{\text {estimate of } U_t} \approx \mathbb{E}_{S_{t+1},A_{t+1}}\left[R_t+\gamma \cdot \underbrace{Q\left(S_{t+1}, A_{t+1} ; \mathbf{w}\right)}_{\text {estimate of } U_{t+1}}\right]
$$

ä¹Ÿå°±æ˜¯ï¼š
$$
\underbrace{Q\left(s_t, a_t ; \mathbf{w}\right)}_{\text {Prediction}} \approx \underbrace{r_t+\gamma \cdot Q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)}_{\text {TD target}}
$$

**!æ³¨æ„**  è¿™é‡Œçš„æ¨å¯¼ä¸ä¸¥è°¨ï¼Œçœç•¥äº†ä¸€äº›æ­¥éª¤ï¼Œè¯¦ç»†è§ä¸‹ä¸€èŠ‚ sarsaç®—æ³• TDå­¦ä¹ 

æ³¨æ„è¿™é‡Œçš„çº¦ç­‰äºå·ï¼Œè¿™è¯´æ˜ç­‰å¼ä¸¤ä¾§æ˜¯ä¸ç›¸ç­‰çš„ã€‚æˆ‘ä»¬æ›´ç›¸ä¿¡TD targetï¼Œå› ä¸ºç›¸è¾ƒäºPredictionï¼ŒTD targetçš„è®¡ç®—åˆ©ç”¨äº†è¿™ä¸€æ­¥å®é™…çš„$r_t$ï¼ŒTD targetæ›´åŠ æ¥è¿‘çœŸç›¸ã€‚

Prediction:
$$Q\left(s_t, a_t ; \mathbf{w}\right)$$

## TD target ä¸¤ç§è®¡ç®—æ–¹å¼:
ä¸€ç§æ˜¯ç”¨ä¸‹ä¸€æ­¥å®é™…çš„$Q$
$$
y_t = \underbrace{r_t+\gamma \cdot Q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)}_{\text {TD target}}
$$
ä¸€ç§æ˜¯ç”¨é‡‡å–ä¸åŒåŠ¨ä½œaå¯ä»¥å¾—åˆ°çš„æœ€å¤§çš„$maxQ()$
$$
y_t = \underbrace{r_t+\gamma \cdot \max _{a} Q\left(s_{t+1}, a ; \mathbf{w}\right)}_{\text {TD target}}
$$

è¿™ä¸¤ç§å…¶å®åˆ†åˆ«å¯¹åº”äº†**on-policy**å’Œ**off-policy**ç­–ç•¥

è®¡ç®—æŸå¤±Loss:

$$L_t=\frac{1}{2}\left[Q\left(s_t, a_t ; \mathbf{w}\right)-y_t\right]^2$$

åšæ¢¯åº¦ä¸‹é™Gradient descent:

$$
\mathbf{w}_{t+1}=\mathbf{w}_t-\left.\alpha \cdot \frac{\partial L}{\partial \mathbf{w}}\right|_{\mathbf{w}=\mathbf{w}_t}
$$

# æ€»ç»“

DQNä½¿ç”¨ç¥ç»ç½‘ç»œ$Q\left(s_t, a_t ; \mathbf{w}\right)$æ¥è¿‘ä¼¼**æœ€ä¼˜**ä»·å€¼å‡½æ•°$Q^{\star}(a,s)$

$$Q^{\star}\left(s_{t}, a_{t}\right)=\max _{\pi} Q_{\pi}\left(s_{t}, a_{t}\right)
=\max _{\pi}\mathbb{E}\left[U_t \mid S_t=s_t, A_t=a_t\right]
$$

$Q\left(s_t, a_t ; \mathbf{w}\right)$ä¸­$\mathbf{w}$æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°

* è¾“å…¥ï¼šç¯å¢ƒçš„è§‚æµ‹$s$
* è¾“å‡º:æ‰€æœ‰åŠ¨ä½œ$a\in\mathcal{A}$çš„è¯„åˆ†

<img src="7.png" height=100% width=100%>