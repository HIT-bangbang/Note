# ä¸€ã€ç­–ç•¥å‡½æ•°è¿‘ä¼¼Policy Function Approximation

## 1.ç­–ç•¥ä»·å€¼å‡½æ•°$\pi (a|s)$ 

$\pi (a|s)$ æ˜¯ä¸€ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°probability density function (PDF)

* è¾“å…¥ï¼šçŠ¶æ€$s$

* è¾“å‡ºï¼šå„ä¸ªå¯èƒ½çš„åŠ¨ä½œè¢«é€‰æ‹©çš„æ¦‚ç‡probabilities for all the actionsï¼Œä¾‹å¦‚ï¼š
  
$$\pi (left|s) = 0.2 $$
$$\pi (right|s) = 0.1 $$
$$\pi (up|s) = 0.7 $$

agentå…·ä½“é‡‡å–å“ªä¸ªåŠ¨ä½œæ˜¯æŒ‰ç…§$\pi (a|s)$è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæŠ½æ ·å¾—åˆ°çš„ã€‚æ¯”å¦‚åœ¨çŠ¶æ€$s$ä¸‹ï¼ŒæŠ½åˆ°åŠ¨ä½œleftçš„æ¦‚ç‡ä¸º0.2ã€‚æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡ä¹‹å’Œä¸º1

The agent performs an action ğ‘ random drawn from the distribution

***

## 2.åŠ¨ä½œç¦»æ•£ä¸”æœ‰é™çš„æƒ…å†µ

<img src="1.png" height=50% width=50%>

***

## 3.ç­–ç•¥ç½‘ç»œPolicy Network $\pi(a|s;\theta)$

* ç­–ç•¥ç½‘ç»œï¼šç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ç­–ç•¥å‡½æ•°$\pi (a|s)$
* å…¶ä¸­$\theta$æ˜¯æˆ‘ä»¬éœ€è¦æ›´æ–°å’Œä¼˜åŒ–çš„ç¥ç»ç½‘ç»œå‚æ•°

ç½‘ç»œè¾“å…¥è¾“å‡ºå¦‚ä¸‹å¦‚æ‰€ç¤ºï¼š

<img src="2.png" height=50% width=50%>

å…¶ä¸­ï¼Œsoftmaxå±‚ä¿è¯äº†æ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡ä¹‹å’Œä¸º1

$$\sum_{a \in \mathcal{A}} \pi\left(\left.a\right|_s ; \boldsymbol{\theta}\right)=1$$

# äºŒã€çŠ¶æ€ä»·å€¼å‡½æ•°è¿‘ä¼¼State-Value Function Approximation

## 1.çŠ¶æ€ä»·å€¼å‡½æ•°$V_{\pi}\left(s_{t}\right)$ å’ŒçŠ¶æ€ä»·å€¼ç½‘ç»œ$V_\pi\left(s_t;\theta\right)$

çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š

$$V_{\pi}\left(s_{t}\right)=\mathbb{E}_{A}\left[Q_{\pi}\left(s_{t}, A\right)\right]$$

$$
\begin{aligned}
& V_\pi\left(s_t\right)=\mathbb{E}_A\left[Q_\pi\left(s_t, A\right)\right]=\sum_a \pi\left(a \mid s_t\right) \cdot Q_\pi\left(s_t, a\right) \\

& V_\pi\left(s_t\right)=\mathbb{E}_A\left[Q_\pi\left(s_t, A\right)\right]=\int_a \pi\left(a \mid s_t\right) \cdot Q_\pi\left(s_t, a\right) da
\end{aligned}
$$

* å«ä¹‰ï¼šå¯¹äºå›ºå®šçš„ç­–ç•¥$\pi$ï¼Œ$V_\pi(s)$è¯„ä¼°äº†å½“å‰çŠ¶æ€çš„å¥½å

ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼è¿™ä¸ªçŠ¶æ€ä»·å€¼å‡½æ•°ï¼š

$V_\pi\left(s_t;\theta\right)=\mathbb{E}_A\left[Q_\pi\left(s_t, A\right)\right]=\sum_a \pi\left(a \mid s_t;\theta\right)\cdot Q_\pi\left(s_t, a\right)$

* æ³¨æ„è¿™é‡Œçš„ $\theta$ æ˜¯ç­–ç•¥ç½‘ç»œ$\pi(s\mid a;\theta)$çš„ $\theta$
* çŠ¶æ€ä»·å€¼å‡½æ•°å–å†³äº$\pi$çš„å¥½å
* Policy-based learningçš„ç»ˆæç›®æ ‡å°±æ˜¯å­¦ä¸€ä¸ªæœ€å¥½çš„ $\theta$ (ä¹Ÿå°±æ˜¯å­¦ä¸€ä¸ªæœ€å¥½çš„$\pi(s\mid a;\theta)$å‡½æ•°)ï¼Œä½¿å¾—å¯¹äºæ‰€æœ‰å¯èƒ½çš„çŠ¶æ€$s$ï¼Œéƒ½æœ‰ä¸€ä¸ªæ€»ä½“ä¸Šæœ€å¥½çš„çŠ¶æ€ä»·å€¼ã€‚
* å¯¹äºåŒæ ·çš„æƒ…å†µï¼Œå¯¹äºä¸€ä¸ªå¾€å¾€é‡‡å–ä¸å¥½çš„ç­–ç•¥çš„æ–°æ‰‹ï¼Œå¯èƒ½æ˜¯ä¸ªæ­»å±€ã€‚ä½†æ˜¯å¯¹äºæœ‰éå¸¸å¥½çš„ç­–ç•¥çš„è€æ‰‹æ¥è¯´ï¼Œå¯èƒ½è½¬å±ä¸ºå®‰ã€‚
* æˆ‘ä»¬çš„ç›®çš„å°±åœ¨äºå­¦åˆ°è¿™ä¸ªæ›´å¥½çš„ç­–ç•¥ï¼Œä½¿å¾—æ‰€æœ‰çš„çŠ¶æ€ä¸‹ï¼ŒçŠ¶æ€ä»·å€¼è¾¾åˆ°æ€»ä½“æœ€ä¼˜ï¼ˆå¯¹äºæ‰€æœ‰çŠ¶æ€çš„æœŸæœ›æœ€å¤§ï¼‰ã€‚

*** 

## 2.ç­–ç•¥å­¦ä¹ Policy-based learning

ç­–ç•¥å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¼˜åŒ–å‚æ•°$\boldsymbol \theta$æœ€å¤§åŒ–çŠ¶æ€ä»·å€¼å‡½æ•°çš„æœŸæœ›$J(\boldsymbol{\theta})$

  $$J(\boldsymbol{\theta})=\mathbb{E}_S[V(S ; \boldsymbol{\theta})]$$

å¦‚ä½•æ›´æ–° $\theta$ ?------->  æ¢¯åº¦ä¸Šå‡Policy gradient ascent

æ­¥éª¤ï¼š

* è§‚å¯Ÿåˆ°çŠ¶æ€$s$
* æ›´æ–°ç­–ç•¥å‡½æ•°çš„å‚æ•°ï¼š$\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\beta \cdot \frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$

è®©æˆ‘ä»¬æ‹ä¸€æ‹è¿™ä¸ªé€»è¾‘ï¼š

æˆ‘ä»¬æœ€ç»ˆçš„ç›®æ ‡è¦å¾—åˆ°çš„æ˜¯æœ€å¥½çš„$\pi(s \mid a ; \boldsymbol \theta)$ï¼Œå®ƒæ˜¯$\boldsymbol \theta$çš„å‡½æ•°ï¼Œ

$\pi(s \mid a ; \boldsymbol \theta)$å¥½ä¸å¥½çš„è¯„ä»·æ ‡å‡†æ˜¯$J(\boldsymbol{\theta})$å¤§ä¸å¤§ï¼Œ

å› ä¸º$J(\boldsymbol{\theta})$æ˜¯$V_\pi\left(s_t;\theta\right)$çš„æœŸæœ›ï¼Œè€Œ$V_\pi\left(s_t;\theta\right)$æ˜¯ä¸$\pi(s \mid a ; \boldsymbol \theta)$æœ‰å…³çš„ï¼Œ$J(\boldsymbol{\theta})$è¶Šå¤§å°±æ˜¯$V_\pi\left(s_t;\theta\right)$è¶Šå¤§ï¼Œå°±æ˜¯$\pi(s \mid a ; \boldsymbol \theta)$è¶Šå¥½

ä¸ºäº†è®©$J(\boldsymbol{\theta})$å˜å¤§ï¼Œå°±å¾—æ‰¾æ›´å¥½çš„$\pi(s \mid a ; \boldsymbol \theta)$ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡æ¢¯åº¦ä¸Šå‡æ›´æ–°$\boldsymbol \theta$


# ä¸‰ã€ç­–ç•¥æ¢¯åº¦$\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$

æ›´æ–°å‚æ•°$\boldsymbol{\theta}$çš„å…³é”®åœ¨äºè®¡ç®—ç­–ç•¥æ¢¯åº¦
$\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$

* æ¨å¯¼è¿‡ç¨‹ç•¥

æœ€ç»ˆç­–ç•¥æ¢¯åº¦æœ‰ä¸¤ç§å½¢å¼ï¼š

**Form 1 ç¦»æ•£å½¢å¼:**
$$
\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\sum_a \frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, a) 
$$
**Form 2 è¿ç»­å½¢å¼:**
$$
\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\mathbb{E}_{A \sim \pi(\cdot \mid s ; \boldsymbol{\theta})}\left[\frac{\partial \log \pi(A \mid s, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, A)\right]
$$

# å››ã€è®¡ç®—ç­–ç•¥æ¢¯åº¦

## 1ã€ç¦»æ•£å½¢å¼

æ¯”å¦‚åŠ¨ä½œç©ºé—´ä¸º$\mathcal{A}=\{"left","right","up",... \}$

**æ³¨æ„ï¼š** $Q_\pi(s, a)$è¯¥æ€ä¹ˆç®—è¿˜ä¸çŸ¥é“ï¼Œè¿™ä¸ªé—®é¢˜åœ¨åé¢å†è§£å†³ï¼Œç°åœ¨å‡è®¾$Q_\pi(s, a)$å·²ç»èƒ½ç®—äº†

ä½¿ç”¨ç¦»æ•£å½¢å¼çš„ç­–ç•¥æ¢¯åº¦è®¡ç®—å…¬å¼Form 1ï¼š

**Form 1 ç¦»æ•£å½¢å¼:**
$$
\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\sum_a \frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, a) 
$$

* 1ã€å¯¹åŠ¨ä½œç©ºé—´å†…çš„æ‰€æœ‰çš„åŠ¨ä½œ $a\in\mathcal{A}$ è®¡ç®— $\mathbf{f}(a, \boldsymbol{\theta})=\frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, a)$
* 2ã€æ±‚å’Œï¼Œå¾—åˆ°ç­–ç•¥æ¢¯åº¦$\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\mathbf{f}(\text { "left", } \boldsymbol{\theta})+\mathbf{f}(\text { "right", } \boldsymbol{\theta})+\mathbf{f}(\text { "up", } \boldsymbol{\theta})$

**å¦‚æœåŠ¨ä½œç©ºé—´å¾ˆå¤§çš„è¯ï¼Œè¿™ç§æ–¹æ³•å°†ä¼šæ˜¯å¾ˆè€—æ—¶çš„**

***

## 1ã€ç¦»æ•£å½¢å¼

åŠ¨ä½œç©ºé—´æ˜¯è¿ç»­çš„ï¼Œä¾‹å¦‚$\mathcal{A}=\left [ 0 , 1 \right ]$

ä½¿ç”¨è¿ç»­å½¢å¼çš„è®¡ç®—å…¬å¼Form 2:

**Form 2 è¿ç»­å½¢å¼:**
$$
\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\mathbb{E}_{A \sim \pi(\cdot \mid s ; \boldsymbol{\theta})}\left[\frac{\partial \log \pi(A \mid s, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, A)\right]
$$

* è¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼ŒåŠ¨ä½œæ˜¯è¿ç»­çš„ï¼Œæœ‰æ— æ•°ä¸ªï¼Œé‚£ä¹ˆå¦‚ä½•æ±‚æœŸæœ›ï¼Ÿæ˜¾ç„¶ä¸èƒ½æ˜¯ç§¯åˆ†ï¼Œå› ä¸ºå¾ˆå¤šå‡½æ•°çš„ç§¯åˆ†æ²¡æ³•æ±‚ï¼Œæˆ–è€…å¾ˆéš¾æ±‚
* è§£å†³åŠæ³•ï¼Œè’™ç‰¹å¡æ´›è¿‘ä¼¼

æ±‚è§£æ­¥éª¤ï¼š

* 1.æ ¹æ®å½“å‰çš„çŠ¶æ€$s$ï¼Œä½¿ç”¨å½“å‰çš„å‚æ•°$\theta$ï¼Œå¾—åˆ°ç­–ç•¥å‡½æ•°çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯åŠ¨ä½œçš„æ¦‚ç‡å¯†åº¦å‡½æ•°$\pi(\cdot \mid s ; \boldsymbol \theta)$

* 2.æŒ‰ç…§è¿™ä¸ªå¾—åˆ°çš„æ¦‚ç‡å¯†åº¦è¿›è¡ŒæŠ½æ ·ï¼Œå¾—åˆ°ä¸€ä¸ªåŠ¨ä½œ $\hat{a}$ 

* 3.è®¡ç®—$\mathbf{g}(\hat{a}, \boldsymbol{\theta})=\frac{\partial \log \pi(\hat{a} \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, \hat{a})$

* 4.ç”¨$\mathbf{g}(\hat{a}, \boldsymbol{\theta})$ä½œä¸ºç­–ç•¥æ¢¯åº¦$\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$çš„è¿‘ä¼¼

è’™ç‰¹å¡æ´›è¿‘ä¼¼çš„è§£é‡Šï¼š

è¿™é‡Œæˆ‘ä»¬å®šä¹‰$\mathbf{g}(A, \boldsymbol{\theta})=\frac{\partial \log \pi(A \mid s, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, A)$

é‚£ä¹ˆ$\mathbb{E}_{A} \left[\mathbf{g}(A, \boldsymbol{\theta})\right] = \frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$

$\mathbf{g}(\hat{a}, \boldsymbol{\theta})$å°±æ˜¯$\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$çš„æ— åä¼°è®¡

# äº”ã€ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ›´æ–°ç­–ç•¥ç½‘ç»œUpdate policy network using policy gradient

<img src="3.png" height=50% width=50%>

* è¿™é‡Œçš„ç¬¬ä¸‰æ­¥è¿˜æ²¡è§£å†³

# å…­ã€å¦‚ä½•è®¡ç®— $Q_\pi\left(s_t, a_t\right)$

## 1ã€REINFORCEç®—æ³•

* ç©å®Œå®Œæ•´çš„ä¸€è½®æ¸¸æˆï¼Œå¾—åˆ°ä¸€æ¡è½¨è¿¹ï¼ˆtrajectoryï¼‰
  $$s_1,a_1,r_1, s_2,a_2,r_2,\space ... \space s_T,a_T,r_T$$
* å¯¹äºæ‰€æœ‰çš„$t$è®¡ç®—æŠ˜æ‰£å›æŠ¥$u_t=\sum_{k=t}^T \gamma^{k-t} r_k$
* å› ä¸ºåŠ¨ä½œä»·å€¼å‡½æ•°$Q$å°±æ˜¯æŠ˜æ‰£å›æŠ¥çš„æœŸæœ›ï¼Œé‚£ä¹ˆå…¶å®å¯ä»¥ç”¨è¿™ä¸€è½®æ¸¸æˆå¾—åˆ°çš„è½¨è®¡ç®—$u_t$æ¥è¿‘ä¼¼$Q_\pi\left(s_t, a_t\right)$
$$ Q_\pi\left(s_t, a_t\right)=\mathbb{E}\left[U_t \mid S_t=s_t, A_t=a_t\right] $$

## 2ã€ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼$Q_\pi$  	$\Rightarrow$  actor-critic æ–¹æ³•


# æ€»ç»“ï¼š

<img src="4.png" height=50% width=50%>

